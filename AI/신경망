신경망(Neural Network) : 뇌구조를 모방한 계산 모형
                        
                        구성 
                        입력층 - 은닉층 - 출력층
                        가중치를 갖는 층은 입력층과 은닉층




1. 퍼셉트론(Perceptron) : 입력은 특징 벡터 X=(x1,x2,...,wx)
                       x를 두개의 부류 w1과 w2중의 하나로 분류하는 이진 분류기
                       출력 노드는 가중치 합과 활성 함수를 계산
                       계단 함수를 활성함수로 사용
                       
                       = 인공뉴런(Artificial Neuron)
                       
                       여러개의 입력된 정보에 대해 가중치 합을 계산하여 출력정보 생성
                       활성화 함수 혹은 전달함수을 통해 전달
                       
                       
                       동작원리
                           { 0 (w1x1 + w2x2 <= 임계값) }
                       y = { 1 (w1x1 + w2x2 > 임계값) }
                       
                       가중치(w)는 전류의 저항과 비슷한 의미 (가중치가 크면 더 강한 신호를 흘려 보냄)
                       뉴런에서 보내온 신호의 총합이 임계값보다 큰 경우 1을 출력 즉 활성화
                       
                       
                       
                       
2. 다층 퍼셉트론(MLP) : 입력층 -> 은닉층 -> 출력층

                    입력층 : 특징 벡터의 차원에 따라 d개의 노드와 여분의 바이어스 노드로 구성
                    출력층 : 부류 개수에 따라 m개의 노드로 구성
                    은닉층 : 노드 개수 p를 사용자가 설정
                    
                    
                    
                    
깊은학습(Deep Learning) : MLP의 은닉층 개수를 늘린 깊은 신경망




활성함수의 종류
1) Sigmoid function(=Logistic function) : 미분이 되지 않는 지점에서 사용하는 함수
                                          계단형식의 함수를 미분이 가능하도록 곡선화 해주는 함수
                                          1과 0 사이를 부드럽게 이어줌
                                          0-1 사이의 값을 반환
                                          연속형 데이터이기에 계단함수가 끊기지 않는 매끄러운 모양
                                          이진분류 가능
                                          
                                          +단점
                                          출력되는 갑의 범위가 매우 좁기 때문에 경사하강법 수행시 범위가 너무 좋아
                                          0에 수렴하는 기울기 소실(Gradient Vanishing)이 발생 할 수 있다.
                                          
                                          +기울기 소실 문제( Gradient Vanishing )
                                          값의 왜곡이라 할 수 없지만 값이 현저하게 줄어들게 됨
                                          출력의 중앙값이 0.5 
                                          모두 양수이기때문에 출력의 가중치 합이 입력의 가중치 합보다 커짐 -> 편향이동(Bias Gradient)
                                          
                                          
                                          
                                          
2) ReLU function ( Rectified Linear Unit ) :  0 이하의 값은 다음 레이어에 전달하지 않음, 0 이상의 값은 그대로 출력
                                             CNN을 학습시킬때 많은 사용
                                             
                                             +단점
                                             한번 0 활성화 값을 다음 레이어에 전달하면 이후의 뉴런 출력값은 모두 0 -> dying ReLU
                                             


3) 쌍곡탄젠트(= 하이퍼볼릭 함수) : RNN, LSTM 등에 사용



3. Linear Regression
   Regression Analysis(회귀분석) : 2개 또는 그 이상 변수들의 의존관계를 파악함으로써 특정변수의 값을 예측하는 통계학의 한 분야
   
   Linear Regression Analysis(선형 회귀분석) : 두 변수 x, y에 대한 n개의 측정값이 있을때
                                              (x1, y1), (x2, y2) ... (xn, yn)
                                              주어진 가설에 대한 비용이 최소화 되도록 하는 직선을 찾는 문제
                    
                    
   Linear Hypothesis : H(x) = Wx + b
   
   Cost Function : cost(W, b)
               
                  + How to minimize cost?
                    경사하강 알고리즘( Gradient descent algorithm ) 이용
                    
                  + 경사하강 알고리즘 (Gradient descent algorithm)?
                    임의의 곳에서 시작하여 경사도(gradient)에 따라 W를 변경시켜가면서 cost 함수의 값이 최소화 되는 W를 구하는 알고리즘
                    
   
   Linear Regression : cost함수 cost(W, b)를 최소화하는 W와 b를 찾는 문제
   
   
   구성단계
   (1) Graph 구성(가설, 비용함수, 학습함수 정의)
      - Hypothesis 정의, Cost/Loss 함수 정의, Train 함수 정의
      
   (2) Training
   
   (3) Testing ...
   
   
   
   + 참고 @tf.function 데코레이터
   @tf.function : 파이썬 문법의 일부를 높은 성능의 텐서플로 그래프 코드로 변환시키기 위해 사용
   
   
   
4. Multiple Linear Regression
   입력데이터가 1개가 아니라 여러 개인 경우
   Hypothesis와 cost함수의 변화 -> Linear Regression과 비교하였을때 입력데이터의 수가 다르기 때문



5. Logistic Regression
   
   Binary Logistic Classification : Class가 2개인 경우
                                   Class(label)은 0과 1로 encoding
                                   
                                   새로운 hypothesis ->  H(x)=1/(1+e−WTX+b)
                                   새로운 Cost Function 
                                   
                                   How to minimize cost?
                                   - 경사하갈 알고리즘 

  

